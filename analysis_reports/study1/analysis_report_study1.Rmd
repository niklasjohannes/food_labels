---
title: "Data Analysis Report Study 1"
author: "Niklas Johannes"
date: "25/11/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      message = FALSE, 
                      warning = FALSE,
                      cache = TRUE)
```

This file explains all data processing and analysis steps for Study 1 of [Project Title].
The RProject has a private library in order to make all steps computationally reproducible.
I use the `renv` package for this.
That means you will need to install the package and run the `renv::restore` command, see instructions [here](https://github.com/rstudio/renv).
```{r load_libraries}
# pacman makes it easier to load and install packages
if (!requireNamespace("pacman"))
  install.packages("pacman")

library(pacman)

# load packages
p_load(
  here,
  janitor,
  pastecs,
  betareg,
  car,
  lattice,
  cowplot,
  lme4,
  afex,
  DHARMa,
  emmeans,
  MuMIn,
  tidyverse
)

# set seed
set.seed(42)

# set theme
theme_set(theme_cowplot())
```

```{r custom_functions}
### FUNCTION 1 ###
# little function that describes and visualizes variables, either for repeated-measures or trait-like
describe_visualize <- 
  function(df, 
           variable, 
           repeated_measure = FALSE,
           want_summary = FALSE){
    
    variable <- enquo(variable)
    
    # specifies whether the variable we want to plot is a trait-like or repeated measure
    if (repeated_measure == FALSE){
      df <-
        df %>%
        group_by(food) %>%
        slice(1) %>% 
        ungroup()
    } else {
      df <-
        df
    }
    
    # descriptive stats
    sum_stats <-
      df %>% 
      pull(!! variable) %>% 
      stat.desc()
    
    # plot
    plot <-
      ggplot(df, aes(x = !! variable)) +
        geom_density(color = "darkgrey", fill = "darkgrey") +
        geom_point(aes(x = !! variable, y = 0))
    
    # return the two (if both are wanted)
    if(want_summary == TRUE){
      return(list(knitr::kable(sum_stats), plot))
    } else{
      return(plot)
    }
  }

### FUNCTION 2 ###
# function to produce violin plots for a variable for each food type
three_violins <-
  function(
    df,
    variable
  ){
    
    variable <- enquo(variable)
    
    df <-
      df %>% 
      group_by(food) %>% 
      slice(1) %>% 
      ungroup()
    
    ggplot(
      df,
      aes(
        x = food_type,
        y = !! variable,
        color = food_type,
        fill = food_type
      )
    ) +
      geom_violin() +
      scale_color_brewer(palette = "Dark2") +
      scale_fill_brewer(palette = "Dark2")
  }

### FUNCTION 3###
# provides summary statistics for a variable for each food type
summary_stats <-
  function(
    df,
    variable
  ){
    variable <- enquo(variable)
    
    summary_stats <-
      df %>%
      group_by(food_type) %>%
      summarize(mean = mean(!! variable),
                sd = sd(!! variable))
    
    summary_stats
  }
```

# 1. Load data and wrangle data
First, we load the data.
For a codebook, see the [enter file name] file.
```{r load_data}
raw_data <- read_csv(
  here("data", "study1", "raw_data.csv")
  )
```

The variable names aren't in a nice format.
Notably, they all contain `_all_items_final` at the end and are in all upper case.
I will first remove the `_all_items_final` part from the variable names and then use the `janitor` package to get nicer variable names.
```{r clean_names}
# use a working file from now on
working_file <- raw_data

# remove variable name part
names(working_file) <-
  names(working_file) %>% 
  str_remove(., "_all_items_final")

# get variable names in snake case
working_file <- 
  clean_names(working_file)

# inspect data
glimpse(working_file)
```

Next, I add a food type category based on the subject number (`subjectnr` indicates the food ID):

* 1-80: vegan
* 81-160: vegetarian
* 161-240: meat-based
```{r add_food_type}
working_file <-
  working_file %>% 
  mutate(
    food_type = case_when(
      subjectnr %in% 1:80 ~ "vegan",
      subjectnr %in% 81:160 ~ "vegetarian",
      subjectnr %in% 161:240 ~ "meat_based"
    )
  ) %>% 
  rename( # give more sensible name for food ID
    "food_id" = "subjectnr"
  )
```

At this point, we also don't have the actual food names in the data set.
Although we don't necessarily need them in the analysis, I'll add them for completeness' sake.
In addition, the file also has the supermarkets that belong to each food, and I'll need those for analysis.

The food label data set is in the long format, meaning I first need to get unique food names, then add them to working file.
```{r add_food_names}
# read the file with food names and features
food_labels <-
  read_csv(
    here("data", "study1", "food_labels.csv")
  )

head(food_labels)

# add food names to raw_data
working_file <-
  working_file %>% # the foods are in exactly the right order
    add_column(
      food = unique(food_labels$food) # file is in the long format
    ) %>% # reorder variable names
  left_join( # then join the supermarkets by food
    .,
    food_labels %>% # long format so that there's only one food entry per food
      group_by(food) %>% 
      slice(1) %>% 
      select(food, supermarket),
    by = c("food")
  ) %>% 
  select(food_id, food, supermarket, everything())
```

Next, I create the total number of features produced per food item and the proportion of features for each item.
```{r total_features}
# add total features per item and remove ambigous and nonwords
working_file <-
  working_file %>%
  select(-ambiguous, -nonword) %>% 
  mutate(
    total = rowSums(select(., production:neg_coping))
  )

# add proportion of each feature in relation to total number of features
total_proportions <-
  working_file %>% 
  mutate_at(
    vars(production:neg_coping),
    list(~ . / total)
  )

# add an identifier of that variable name
names(total_proportions)[4:46] <-
  names(total_proportions %>% 
          select(production:neg_coping)) %>% 
  str_replace(., "$", "_proportion")

# add those proportion variables to working file
working_file <-
  left_join(
    working_file,
    total_proportions
  )
```

I repeat these two steps for the three main categories: consumption situations, non-consumption situations, and situation-independent situations.
```{r per_category_features}
# get total number of features per category
working_file <-
  working_file %>% 
    mutate(
      consumption_total = rowSums(select(., taste_flavor:neg_coping)),
      non_consumption_total = rowSums(select(., production:prep_stor, cult_embed)),
      situation_independent_total = rowSums(select(., ingred_cont:long_ter_neg, overall_pos_eval:linguistic))
    )

# now proportion of features in that category
category_proportions <-
  working_file %>% 
    mutate_at(
      vars(
        consumption_total,
        non_consumption_total,
        situation_independent_total
      ),
      list(~ . / total)
    ) %>% select(food_id, consumption_total:situation_independent_total)

# add an identifier for those variables
names(category_proportions)[2:4] <-
  names(category_proportions %>% 
  select(-food_id)) %>% 
  str_replace(., "total", "proportion")

# add those proportion variables to working file
working_file <-
  left_join(
    working_file,
    category_proportions,
    by = c("food_id")
  )
```

Last, I do the same for the different sub-categories of consumption situations: sensory & reward, immediate positive consequences, immediate negative consequences, and contextual features.
```{r per_sub_category_features}
# get total number of features per sub-category
working_file <-
  working_file %>% 
    mutate(
      sensory_total = rowSums(select(., taste_flavor:action)),
      positive_total = rowSums(select(.,
                                      pos_conform_goals,
                                      pos_social_goals,
                                      pos_bodily_conseq:pos_coping)),
      negative_total = rowSums(select(.,
                                neg_conform_goals,
                                neg_social_goals,
                                neg_bodily_conseq:neg_coping)),
      contextual_total = rowSums(select(., cont_bodily:cont_consumable))
    )

# now proportion of features in that category
sub_category_proportions <-
  working_file %>% 
    mutate_at(
      vars(sensory_total:contextual_total),
      list(~ . / total)
    ) %>% select(food_id, sensory_total:contextual_total)

# add an identifier for those variables
names(sub_category_proportions)[2:5] <-
  names(sub_category_proportions %>% 
  select(-food_id)) %>% 
  str_replace(., "total", "proportion")

# add those proportion variables to working file
working_file <-
  left_join(
    working_file,
    sub_category_proportions,
    by = c("food_id")
  )
```

Okay, now the file contains all information, but is not following tidy data conventions.
I'll transform it to long format with one observation per row and the proportion data as constants.
```{r tidy_data}
# first add an identifier to the feature count variables
names(working_file)[4:46] <-
  names(working_file %>% 
          select(production:neg_coping)) %>% 
  str_replace(., "$", "_count")

# then pivot to longer
working_file <-
  working_file %>% 
    pivot_longer(
      cols = c(production_count:neg_coping_count, production_proportion:neg_coping_proportion),
      names_to = c("feature", ".value"), # period to tell tidyr that that's where the value is
      names_sep = "_(?!.*_)", # looks for last underscore as the separator (which separates count and proportion)
      values_drop_na = TRUE
    ) %>% 
    select( #reorder variables
      food_id,
      food,
      food_type,
      supermarket,
      feature:proportion,
      total,
      consumption_total:situation_independent_total,
      sensory_total:contextual_total,
      consumption_proportion:situation_independent_proportion,
      sensory_proportion:contextual_proportion
    ) %>% # assign proper variable types
    mutate_at(
      vars(contains("food"), feature),
      list(~ as.factor(.))
    )

# summary stats
working_file %>% 
  group_by(food) %>% 
  slice(1) %>% 
  ungroup() %>% 
  summarize_at(
    vars(
      sensory_proportion,
      positive_proportion,
      contextual_proportion
    ),
    list(mean = mean, median = median, sd = sd)
  )
```

# 2. Describe and visualize
Next, I visualize the different variables.
I'll start with the total number of features overall and per food.
Overall, it looks skewed with a long left tail, but without a really clear visual outlier.

Also, there weren't enough vegetarian options at Iceland and not enough vegan options at M&S, which is why the food types are slightly unbalanced.
```{r visualize_totals_per_food}
# number of features per food and feature type
ggplot(
  data = working_file,
  aes(
    x = reorder(food, count),
    y = count
  )
) +
  geom_bar(
    stat = "identity"
  )

# distribution of total features
ggplot(data = working_file %>% group_by(food) %>% slice(1),
       aes(
         x = total
       )) + 
  geom_density(color = "darkgrey", 
               fill = "darkgrey")

# summary stats
working_file %>%
  group_by(food) %>% 
  slice(1) %>% 
  ungroup() %>% 
  pull(total) %>% 
  stat.desc()

# number of foods per supermarket and food type
working_file %>% 
  group_by(food) %>% 
  slice(1) %>% 
  ungroup() %>% 
  with(.,
       table(
         supermarket,
         food_type
       ))
```

Next, I'll do the same with the proportions proportions of consumption situtation features.
Looks pretty much like a binomial or beta distribution.
One food didn't have any of those features in its labels.
```{r visualize_consumption_features}
describe_visualize(
  working_file,
  consumption_proportion,
  FALSE,
  TRUE
)
```

Similar picture for non-consupmtion situations, but with more mass toward the middle, reflected in higher mean.
```{r visualize_non_consumption_features}
describe_visualize(
  working_file,
  non_consumption_proportion,
  FALSE,
  TRUE
)
```

Situation-independent features have a different distribution with a strong left skew.
```{r visualize_independent_features}
describe_visualize(
  working_file,
  situation_independent_proportion,
  FALSE,
  TRUE
)
```

Last, I want to look at the three sub-categories of consumption features (negative immediate consequences didn't have any features, which is why I leave it out from now on).
```{r visualize_sub_features}
working_file %>% 
  # only select foods and the four sub-categories
  select(
    food_id,
    food,
    sensory_proportion,
    positive_proportion,
    contextual_proportion
  ) %>% 
  group_by(food) %>% 
  slice(1) %>% 
  ungroup() %>% 
  # turn into long format for faceting later
  pivot_longer(
    cols = c(-food_id, -food),
    names_to = c("feature_type", ".value"),
    names_sep = "_"
  ) %>% 
  ggplot(
    aes(x = proportion)
  ) +
  geom_density(color = "darkgrey", fill = "darkgrey") + 
  facet_wrap(~ feature_type)
```

Now per food type.
```{r visualize_totals_per_food_type}
three_violins(working_file, total)
summary_stats(working_file, total)
```

Alright, next I visualize the three main feature categories by food category, starting with consumption features.
```{r consumption_by_food_type}
three_violins(working_file, consumption_proportion)
summary_stats(working_file, consumption_proportion)
```

Next non-consumption features.
```{r non_consumption_by_food_type}
three_violins(working_file, non_consumption_proportion)
summary_stats(working_file, non_consumption_proportion)
```

Followed by situation-independent features.
```{r independent_by_food_type}
three_violins(working_file, situation_independent_proportion)
summary_stats(working_file, situation_independent_proportion)
```

The same for the four sub-categories, starting with sensory and action features.
```{r sensory_by_food_type}
three_violins(working_file, sensory_proportion)
summary_stats(working_file, sensory_proportion)
```

Next: Positive immediate consequences.
```{r positive_by_food_type}
three_violins(working_file, positive_proportion)
summary_stats(working_file, positive_proportion)
```

Contexutal features.
```{r contextual_by_food_type}
three_violins(working_file, contextual_proportion)
summary_stats(working_file, contextual_proportion)
```

Total features by supermarket show that we should take supermarket into account when running our models, as there are small differences in the total number of features between supermarkets.
```{r all_by_supermarket}
# summary stats
working_file %>% 
  group_by(supermarket) %>% 
  summarize(
    mean = mean(total),
    sd = sd(total)
  )

# visualize
ggplot(
  working_file,
  aes(
    x = supermarket,
    y = total,
    color = supermarket,
    fill = supermarket
    )
) +
  geom_violin() +
  scale_color_brewer(palette = "Dark2") +
  scale_fill_brewer(palette = "Dark2")
```


# 3. Analysis

## 3.1 Confirmatory

Next, I see whether there are differences between the food categories on the proportions of features.
We had one hypothesis: Meat-based foods will have more sensory and action features than vegan or vegetarian foods.

First: an analysis file that only contains one line per food.
```{r analysis_file}
analysis_file <-
  working_file %>% 
  group_by(food) %>% 
  slice(1) %>% 
  ungroup() %>% 
  select(-feature) %>% 
  arrange(food_id) %>% 
  mutate(
    supermarket = as.factor(supermarket)
  )
```

We are analyzing proportions, meaning that a general linear model assuming a Gaussian distribution will most likely lead to biased effects. 
However, inspecting the proportions shows that most of them are following a binomial or beta distribution.

Moreover, we saw that there were differences in the total number of features between supermarkets, so I should take that into account in the analysis.
I will therefore run a binomial model that includes a random intercept and slope per supermarket.
```{r sensory_model}
# set contrasts
options(contrasts = c("contr.sum", "contr.poly"))

# check contrasts
contrasts(analysis_file$food_type)

# run model
sensory_model <-
  lme4::glmer(
    cbind(sensory_total, total-sensory_total) ~
      food_type +
      (1 + food_type | supermarket),
    data = analysis_file,
    family = binomial(logit)
  )

# summary
summary(sensory_model)

# approximate effect size
r.squaredGLMM(sensory_model)
```

Next, I use the `DHARMA` package to inspect model diagnostics, which simulates new data for each observed data point and compares that density function to the observed value.
Ideally, the distribution of the residuals is flat at 0.5.

The model diagnostics look really good, it doesn't seem like there's a need to worry about poor fit.
One simulated value has a residual of 1, but the test is nonsignificant, meaning we probably don't need to worry about outliers.
```{r sensory_model_diagnostics}
# simulate residuals
sensory_residuals <- simulateResiduals(sensory_model, 1000)

# plot them
plot(sensory_residuals)

# check for outliers
testOutliers(sensory_residuals, plot = T)
```

Last, I check whether the differences between the three groups are singnificant.
The overall effect misses the cut-off, and the pairwise comparisons show that only vegetarian and meat-based might differ.
Note that I didn't correct for pairwise comparison, because we only have one factor with three levels, see explanation [here](https://alexanderetz.com/2014/09/14/the-special-one-way-anova-or-shutting-up-reviewer-2/).
```{r sensory_model_p}
# p-value with LRT
sensory_model_p <-
  mixed(
    cbind(sensory_total, total-sensory_total) ~
      food_type +
      (1 + food_type | supermarket),
    data = analysis_file,
    family = binomial(logit),
    method = "LRT"
  )

# summary
anova(sensory_model_p)

# pairwise comparison
emmeans(sensory_model, pairwise ~ food_type, adj = c("none"))
```

## 3.2 Exploratory

### 3.2.1 Consumption
Next, I'd like to explore whether there are any differences between food types on the three main situation categories.
Just like above, I'll start with consumption features.
```{r consumption_model}
# run model
consumption_model <-
  lme4::glmer(
    cbind(consumption_total, total-consumption_total) ~
      food_type +
      (1 + food_type | supermarket),
    data = analysis_file,
    family = binomial(logit)
  )

# summary
summary(consumption_model)

# approximate effect size
r.squaredGLMM(consumption_model)
```

The model diagnostics look fine.
The test for outliers barely passes the significance cut-off, but inspecting the actual plots shows that the model fits rather well.
```{r consumption_model_diagnostics}
# simulate residuals
consumption_residuals <- simulateResiduals(consumption_model, 1000)

# plot them
plot(consumption_residuals)

# check for outliers
testOutliers(consumption_residuals, plot = T)
```

Next, I inspect the p-value and pairwise comparisons.
The overall effect is significant, again driven by highly significant difference between vegetarian and meat-based.
```{r consumption_model_p}
# p-value with LRT
consumption_model_p <-
  mixed(
    cbind(consumption_total, total-consumption_total) ~
      food_type +
      (1 + food_type | supermarket),
    data = analysis_file,
    family = binomial(logit),
    method = "LRT"
  )

# summary
anova(consumption_model_p)

# pairwise comparison
emmeans(consumption_model_p, pairwise ~ food_type, adj = c("none"))
```

### 3.2.2 Non-consumption

I'll continue with non-consumption features.
The model converges without problems.
```{r non_consumption_model}
# run model
non_consumption_model <-
  lme4::glmer(
    cbind(non_consumption_total, total-non_consumption_total) ~
      food_type +
      (1 + food_type | supermarket),
    data = analysis_file,
    family = binomial(logit)
  )

# summary
summary(non_consumption_model)

# approximate effect size
r.squaredGLMM(non_consumption_model)
```

The model diagnostics look fine again.
The test for outliers passes the significance cut-off, but inspecting the actual plots shows that the model fits rather well.
```{r non_consumption_model_diagnostics}
# simulate residuals
non_consumption_residuals <- simulateResiduals(non_consumption_model, 1000)

# plot them
plot(non_consumption_residuals)

# check for outliers
testOutliers(non_consumption_residuals, plot = T)
```

Next, I inspect the p-value and pairwise comparisons.
The overall effect is significant, driven by highly significant difference between vegan and meat-based.
The difference between vegan and vegetarian is also significant.
```{r non_consumption_model_p}
# p-value with LRT
non_consumption_model_p <-
  mixed(
    cbind(non_consumption_total, total-non_consumption_total) ~
      food_type +
      (1 + food_type | supermarket),
    data = analysis_file,
    family = binomial(logit),
    method = "LRT"
  )

# summary
anova(non_consumption_model_p)

# pairwise comparison
emmeans(non_consumption_model_p, pairwise ~ food_type, adj = c("none"))
```

### 3.2.3 Situation independent

I'll continue with situation-independent features.
The model converges without problems.
```{r situation_independent_model}
# run model
situation_independent_model <-
  lme4::glmer(
    cbind(situation_independent_total, total-situation_independent_total) ~
      food_type +
      (1 + food_type | supermarket),
    data = analysis_file,
    family = binomial(logit)
  )

# summary
summary(situation_independent_model)

# approximate effect size
r.squaredGLMM(situation_independent_model)
```

The model diagnostics look really good.
One outlier is flagged, but the overall outlier statistic shows that this is nothing to worry about.
```{r situation_independent_model_diagnostics}
# simulate residuals
situation_independent_residuals <- simulateResiduals(situation_independent_model, 1000)

# plot them
plot(situation_independent_residuals)

# check for outliers
testOutliers(situation_independent_residuals, plot = T)
```

Next, I inspect the p-value and pairwise comparisons.
The overall effect is barely significant, driven by highly significant difference between vegetarian and meat-based foods; the difference between vegan and meat-based is also significant.
```{r situation_independent_model_p}
# p-value with LRT
situation_independent_model_p <-
  mixed(
    cbind(situation_independent_total, total-situation_independent_total) ~
      food_type +
      (1 + food_type | supermarket),
    data = analysis_file,
    family = binomial(logit),
    method = "LRT"
  )

# summary
anova(situation_independent_model_p)

# pairwise comparison
emmeans(situation_independent_model_p, pairwise ~ food_type, adj = c("none"))
```

# 4. Write files
Last, I write the analysis files.
```{r write_files}
write_csv(analysis_file, here("data", "study1", "analysis_file.csv"))
```
